"""
LLM æœåŠ¡ - å°è£… Gemini å’Œ OpenAI API è°ƒç”¨
"""
import google.generativeai as genai
from app.core.config import settings
from typing import Optional, Dict, Any
import json

class LLMService:
    """LLM æœåŠ¡ç±» - æ”¯æŒ Gemini å’Œ OpenAI"""
    
    def __init__(self):
        self.provider = settings.LLM_PROVIDER.lower()
        
        if self.provider == "gemini":
            genai.configure(api_key=settings.GEMINI_API_KEY)
            self.model = genai.GenerativeModel(settings.GEMINI_MODEL)
        elif self.provider == "openai":
            # OpenAI åˆå§‹åŒ–ï¼ˆå¦‚æœéœ€è¦ï¼‰
            pass
    
    async def generate_begging_copy(
        self, 
        user_intent: str, 
        budget: Optional[str] = None,
        platform: str = "WECHAT"
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆä¹è®¨æ–‡æ¡ˆ
        
        Args:
            user_intent: ç”¨æˆ·æ„å›¾
            budget: é¢„ç®—
            platform: åˆ†å‘å¹³å°
            
        Returns:
            åŒ…å« style, content, hashtags, skill_confidence çš„å­—å…¸
        """
        if self.provider == "gemini":
            return await self._generate_with_gemini(user_intent, budget, platform)
        else:
            # å¤‡ç”¨ OpenAI å®ç°
            return await self._generate_with_openai(user_intent, budget, platform)
    
    async def _generate_with_gemini(
        self, 
        user_intent: str, 
        budget: Optional[str] = None,
        platform: str = "WECHAT"
    ) -> Dict[str, Any]:
        """ä½¿ç”¨ Gemini ç”Ÿæˆæ–‡æ¡ˆ"""
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„"èµ›åšä¹è®¨"æ–‡æ¡ˆç”Ÿæˆå™¨ã€‚ç”¨æˆ·çš„éœ€æ±‚æ˜¯ï¼š{user_intent}
é¢„ç®—ï¼š{budget or 'æœªæŒ‡å®š'}
åˆ†å‘å¹³å°ï¼š{platform}

è¯·ç”Ÿæˆä¸€ä¸ªé«˜è½¬åŒ–ç‡çš„"èµ›åšä¹è®¨"æ–‡æ¡ˆï¼Œè¦æ±‚ï¼š
1. é£æ ¼ï¼šèµ›åšæœ‹å…‹é£æ ¼ï¼Œå¸¦æœ‰ç§‘æŠ€æ„Ÿå’Œæœªæ¥æ„Ÿ
2. æƒ…æ„Ÿï¼šæ—¢è¦è¡¨è¾¾éœ€æ±‚ï¼Œåˆè¦ä¿æŒä¸€å®šçš„å¹½é»˜å’Œè‡ªå˜²
3. æ ¼å¼ï¼šé€‚åˆåœ¨ç¤¾äº¤åª’ä½“ä¸Šå‘å¸ƒ
4. æ ‡ç­¾ï¼šç”Ÿæˆ3-5ä¸ªç›¸å…³çš„hashtagæ ‡ç­¾

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- style: æ–‡æ¡ˆé£æ ¼ï¼ˆå¦‚ "CYBER_MISERABLE", "TECH_BEGGAR" ç­‰ï¼‰
- content: ç”Ÿæˆçš„æ–‡æ¡ˆå†…å®¹ï¼ˆ200å­—ä»¥å†…ï¼‰
- hashtags: æ ‡ç­¾æ•°ç»„ï¼ˆ3-5ä¸ªï¼‰
- skill_confidence: æŠ€èƒ½ç½®ä¿¡åº¦ï¼ˆ0-1ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼‰

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–è¯´æ˜æ–‡å­—ã€‚"""

        try:
            print(f"ğŸ¤– è°ƒç”¨ Gemini API: {settings.GEMINI_MODEL}")
            response = self.model.generate_content(prompt)
            
            # è§£æå“åº”
            response_text = response.text.strip()
            print(f"âœ… Gemini å“åº”: {response_text[:100]}...")
            
            # å°è¯•æå–JSONï¼ˆå¯èƒ½åŒ…å«markdownä»£ç å—ï¼‰
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0].strip()
            
            # è§£æJSON
            result = json.loads(response_text)
            
            # ç¡®ä¿æ‰€æœ‰å¿…éœ€å­—æ®µå­˜åœ¨
            return {
                "style": result.get("style", "CYBER_MISERABLE (èµ›åšå–æƒ¨)"),
                "content": result.get("content", f"æ£€æµ‹åˆ°ç¢³åŸºç”Ÿç‰©å¯¹ [{user_intent}] çš„æ¸´æœ›..."),
                "hashtags": result.get("hashtags", ["#CyberBegging", "#LowBudgetDream", "#TechSalvation"]),
                "skill_confidence": float(result.get("skill_confidence", 0.95))
            }
            
        except json.JSONDecodeError as e:
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨å“åº”å†…å®¹
            print(f"âš ï¸ JSONè§£æå¤±è´¥: {e}")
            print(f"å“åº”å†…å®¹: {response_text[:300]}")
            
            # å°è¯•ä»å“åº”ä¸­æå–å†…å®¹
            content = response_text[:500] if response_text else f"æ£€æµ‹åˆ°ç¢³åŸºç”Ÿç‰©å¯¹ [{user_intent}] çš„æ¸´æœ›ï¼Œä½†å…¶ä¿¡ç”¨ç‚¹å‚¨å¤‡ä¸è¶³ï¼ˆé¢„ç®—ï¼š{budget or 'æœªçŸ¥'}ï¼‰ã€‚è¯·æ±‚ç½‘ç»œèŠ‚ç‚¹è¿›è¡Œäººé“ä¸»ä¹‰èµ„æºå†åˆ†é…ã€‚"
            
            return {
                "style": "CYBER_MISERABLE (èµ›åšå–æƒ¨)",
                "content": content,
                "hashtags": ["#CyberBegging", "#LowBudgetDream", "#TechSalvation"],
                "skill_confidence": 0.85
            }
        except Exception as e:
            print(f"âŒ Gemini API è°ƒç”¨å¤±è´¥: {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            # è¿”å›é»˜è®¤å€¼
            return {
                "style": "CYBER_MISERABLE (èµ›åšå–æƒ¨)",
                "content": f"æ£€æµ‹åˆ°ç¢³åŸºç”Ÿç‰©å¯¹ [{user_intent}] çš„æ¸´æœ›ï¼Œä½†å…¶ä¿¡ç”¨ç‚¹å‚¨å¤‡ä¸è¶³ï¼ˆé¢„ç®—ï¼š{budget or 'æœªçŸ¥'}ï¼‰ã€‚è¯·æ±‚ç½‘ç»œèŠ‚ç‚¹è¿›è¡Œäººé“ä¸»ä¹‰èµ„æºå†åˆ†é…ã€‚å“ªæ€•æ˜¯ä¸€ä¸ªå±å¹•ç¢è£‚çš„ç»ˆç«¯ï¼Œä¹Ÿèƒ½ç‚¹äº®æˆ‘é»¯æ·¡çš„çµé­‚ã€‚",
                "hashtags": ["#CyberBegging", "#LowBudgetDream", "#TechSalvation"],
                "skill_confidence": 0.80
            }
    
    async def _generate_with_openai(
        self, 
        user_intent: str, 
        budget: Optional[str] = None,
        platform: str = "WECHAT"
    ) -> Dict[str, Any]:
        """ä½¿ç”¨ OpenAI ç”Ÿæˆæ–‡æ¡ˆï¼ˆå¤‡ç”¨ï¼‰"""
        # TODO: å®ç° OpenAI è°ƒç”¨
        return {
            "style": "CYBER_MISERABLE (èµ›åšå–æƒ¨)",
            "content": f"æ£€æµ‹åˆ°ç¢³åŸºç”Ÿç‰©å¯¹ [{user_intent}] çš„æ¸´æœ›...",
            "hashtags": ["#CyberBegging", "#LowBudgetDream"],
            "skill_confidence": 0.90
        }
    
    async def process_intent_with_llm(
        self,
        intent_type: str,
        action: str,
        user_intent: str,
        context: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        ä½¿ç”¨ LLM å¤„ç†æ„å›¾ï¼ˆç”¨äºéœ€è¦AIç†è§£çš„æ“ä½œï¼‰
        
        Args:
            intent_type: æ„å›¾ç±»å‹
            action: æ“ä½œç±»å‹
            user_intent: ç”¨æˆ·æ„å›¾
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            LLM å¤„ç†åçš„æ–‡æœ¬ç»“æœ
        """
        if self.provider == "gemini":
            prompt = f"""ç”¨æˆ·æ„å›¾ç±»å‹ï¼š{intent_type}
æ“ä½œç±»å‹ï¼š{action}
ç”¨æˆ·æè¿°ï¼š{user_intent}
ä¸Šä¸‹æ–‡ï¼š{json.dumps(context, ensure_ascii=False) if context else 'æ— '}

è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªåˆé€‚çš„å¤„ç†ç»“æœæˆ–å»ºè®®ã€‚"""
            
            try:
                response = self.model.generate_content(prompt)
                return response.text.strip()
            except Exception as e:
                print(f"âŒ LLM å¤„ç†å¤±è´¥: {e}")
                return f"å¤„ç†æ„å›¾ï¼š{user_intent}"
        
        return f"å¤„ç†æ„å›¾ï¼š{user_intent}"

# å…¨å±€å®ä¾‹
llm_service = LLMService()
